## 1. Fundamental Concept

### a) What are the two phases of the Two-Phase Locking (2PL) protocol, and how do they ensure serializability?

The Two-Phase Locking (2PL) protocol is a concurrency control mechanism used in database systems to ensure serializability of transactions. It consists of two distinct phases:

1. **Growing Phase**:
   - In this phase, a transaction can acquire locks on data items but cannot release any locks.
   - The transaction requests and obtains locks as needed to read or write data items.
   - Once a lock is released, the transaction cannot request any new locks.

2. **Shrinking Phase**:
   - In this phase, a transaction can release locks but cannot acquire any new locks.
   - The transaction gradually releases the locks it holds as it completes its operations.
   - Once the shrinking phase begins, the transaction cannot request additional locks.

### How 2PL Ensures Serializability:
- **Strict Locking Rules**: By enforcing that all locks are acquired before any are released, 2PL prevents transactions from interfering with each other in a way that could lead to inconsistent results.
- **Prevents Cyclic Dependencies**: The protocol ensures that no cycle of dependencies between transactions can form, which is a necessary condition for serializability.
- **Orderly Execution**: Transactions are effectively serialized in the order they enter the shrinking phase, ensuring a consistent and correct execution sequence.

By adhering to these two phases, 2PL guarantees that the execution of concurrent transactions is equivalent to some serial execution, thereby maintaining database consistency and correctness.

---

### b) Why does Two-Phase Locking prevent certain types of anomalies in concurrent transactions, and can it lead to deadlocks? If so, how?

Two-Phase Locking (2PL) prevents certain types of anomalies in concurrent transactions by enforcing strict rules on how locks are acquired and released. Here's how it works and why it prevents anomalies:

### Prevention of Anomalies:
1. **Dirty Reads**:
   - 2PL prevents dirty reads by ensuring that a transaction cannot read data that has been modified by another uncommitted transaction. This is achieved by requiring exclusive locks for write operations and shared locks for read operations.

2. **Non-Repeatable Reads**:
   - By holding locks until the transaction completes, 2PL ensures that once a transaction reads a data item, no other transaction can modify it until the first transaction is finished. This prevents non-repeatable reads.

3. **Phantom Reads**:
   - 2PL can prevent phantom reads by locking ranges of data or using predicate locks, ensuring that no new rows that match a transaction's query criteria can be inserted by other transactions until the first transaction completes.

### Deadlocks in 2PL:
While 2PL effectively prevents anomalies, it can lead to deadlocks. A deadlock occurs when two or more transactions are waiting indefinitely for one another to release locks on resources. Here's how deadlocks can happen in 2PL:

1. **Circular Wait**:
   - Transaction T1 holds a lock on data item A and requests a lock on data item B.
   - Simultaneously, Transaction T2 holds a lock on data item B and requests a lock on data item A.
   - Both transactions are waiting for each other to release locks, resulting in a deadlock.

2. **Lock Ordering**:
   - If transactions request locks in different orders, the likelihood of deadlocks increases. For example, T1 locks A then B, while T2 locks B then A.

### Handling Deadlocks:
Database systems typically handle deadlocks using one or more of the following strategies:
- **Detection and Resolution**:
  - The system periodically checks for deadlocks using a wait-for graph. If a deadlock is detected, one of the involved transactions is aborted to break the cycle.
- **Prevention**:
  - Techniques like lock ordering (always acquiring locks in a predefined order) or using timeouts can help prevent deadlocks.
- **Avoidance**:
  - The system can use algorithms to ensure that transactions do not enter a state where a deadlock is possible.

In summary, while 2PL is effective in preventing anomalies by enforcing strict locking rules, it can lead to deadlocks due to circular waits and inconsistent lock ordering. Proper deadlock handling mechanisms are essential to maintain system performance and reliability.

----

### c) How do version control mechanisms handle concurrent read and write operations to maintain consistency in a database?

Version control mechanisms, often referred to as Multi-Version Concurrency Control (MVCC), handle concurrent read and write operations in a database by maintaining multiple versions of data items. This approach allows readers to access a consistent snapshot of the database without blocking writers, and vice versa. Here's how MVCC works to maintain consistency:

### Key Concepts of MVCC:
1. **Versions of Data**:
   - Each time a data item is modified, a new version of that item is created. The old version is retained to serve ongoing read operations.
   - Versions are typically timestamped or tagged with transaction identifiers to keep track of their validity.

2. **Snapshots**:
   - A transaction sees a snapshot of the database as it was at the start of the transaction. This snapshot includes all committed changes up to that point and excludes any uncommitted changes.

3. **Read and Write Separation**:
   - Readers access the appropriate version of the data based on their snapshot, ensuring they see a consistent view.
   - Writers create new versions of data items, which become visible to new transactions once the write operation is committed.

### How MVCC Handles Concurrent Operations:
1. **Concurrent Reads**:
   - When a transaction reads a data item, it accesses the version that was most recently committed at the time the transaction started.
   - This allows multiple readers to access the database simultaneously without blocking each other.

2. **Concurrent Writes**:
   - When a transaction writes to a data item, it creates a new version of that item. The old version remains available for ongoing read operations.
   - Writers do not block readers, and readers do not block writers, allowing for high concurrency.

3. **Commit and Visibility**:
   - Once a transaction commits, its changes become visible to new transactions. The new versions of the data items are now part of the consistent snapshot for those transactions.
   - Uncommitted changes remain invisible to other transactions until the commit is successful.

### Consistency Mechanisms:
1. **Transaction Isolation Levels**:
   - MVCC supports different isolation levels (e.g., Read Committed, Repeatable Read, Serializable) by controlling how transactions see the versions of data.
   - Higher isolation levels provide stronger consistency guarantees by restricting the visibility of uncommitted changes and ensuring repeatable reads.

2. **Garbage Collection**:
   - Old versions of data items that are no longer needed (i.e., not visible to any active transactions) are cleaned up by a garbage collection process to free up space.

3. **Conflict Detection**:
   - MVCC systems detect conflicts between transactions, especially in higher isolation levels. If a conflict is detected (e.g., a write-write conflict), one of the transactions may be aborted to maintain consistency.

### Benefits of MVCC:
- **High Concurrency**: Readers and writers do not block each other, leading to better performance in highly concurrent environments.
- **Consistent Snapshots**: Transactions operate on a consistent view of the database, ensuring data integrity.
- **Reduced Locking Overhead**: MVCC reduces the need for locks, minimizing contention and improving throughput.

In summary, version control mechanisms like MVCC handle concurrent read and write operations by maintaining multiple versions of data items, allowing transactions to access consistent snapshots of the database. This approach enhances concurrency and consistency without the overhead of traditional locking mechanisms.

---

### d) What are the advantages and potential drawbacks of using version control for managing concurrent transactions compared to traditional locking mechanisms?

Using version control mechanisms, such as Multi-Version Concurrency Control (MVCC), for managing concurrent transactions offers several advantages over traditional locking mechanisms. However, it also comes with potential drawbacks. Here's a detailed comparison:

### Advantages of MVCC:

1. **Improved Concurrency**:
   - **Readers and Writers**: MVCC allows readers to access consistent snapshots of the database without blocking writers, and vice versa. This leads to higher concurrency and better performance in read-heavy environments.
   - **No Writer Blocking**: Writers do not block readers, and readers do not block writers, reducing contention and improving throughput.

2. **Consistent Snapshots**:
   - **Transaction Isolation**: MVCC provides consistent snapshots of the database for each transaction, ensuring that transactions see a consistent view of the data regardless of concurrent modifications.
   - **Isolation Levels**: MVCC supports various isolation levels, allowing flexibility in balancing consistency and performance.

3. **Reduced Locking Overhead**:
   - **Minimized Lock Contention**: By maintaining multiple versions of data items, MVCC reduces the need for locks, minimizing contention and the risk of deadlocks.
   - **Lower Lock Management Costs**: The overhead associated with lock management (e.g., acquiring, releasing, and detecting deadlocks) is significantly reduced.

4. **Better Performance in Read-Heavy Workloads**:
   - **Read-Only Transactions**: Read-only transactions can proceed without any locking overhead, making MVCC particularly efficient for workloads with a high proportion of read operations.

### Potential Drawbacks of MVCC:

1. **Storage Overhead**:
   - **Multiple Versions**: MVCC maintains multiple versions of data items, which can lead to increased storage requirements. Old versions must be retained until no active transactions need them.
   - **Garbage Collection**: Efficient garbage collection mechanisms are required to clean up old versions, which can add complexity and overhead.

2. **Write Amplification**:
   - **Version Creation**: Each write operation creates a new version of the data item, which can lead to write amplification, especially in write-intensive workloads.
   - **Index Maintenance**: Indexes may need to be updated to reflect new versions, adding to the write overhead.

3. **Complexity**:
   - **Implementation**: MVCC is more complex to implement compared to traditional locking mechanisms. It requires careful management of versions, snapshots, and garbage collection.
   - **Conflict Detection**: Detecting and resolving conflicts, especially in higher isolation levels, can add to the complexity.

4. **Potential for Longer Transaction Times**:
   - **Version Retention**: Long-running transactions can cause old versions to be retained for extended periods, increasing storage requirements and potentially impacting performance.
   - **Snapshot Maintenance**: Maintaining consistent snapshots for long-running transactions can be challenging and may require additional mechanisms to ensure correctness.

5. **Isolation Level Trade-offs**:
   - **Weaker Isolation**: While MVCC provides strong consistency guarantees, some isolation levels (e.g., Read Committed) may allow anomalies that stricter locking mechanisms would prevent.
   - **Serializable Isolation**: Achieving serializable isolation with MVCC can be complex and may require additional mechanisms like predicate locking or conflict detection.

### Summary:

- **Advantages**: MVCC offers improved concurrency, consistent snapshots, reduced locking overhead, and better performance in read-heavy workloads.
- **Drawbacks**: It can lead to increased storage overhead, write amplification, implementation complexity, potential for longer transaction times, and trade-offs in isolation levels.

In summary, MVCC is highly effective for environments with high read concurrency and can significantly improve performance and scalability. However, it requires careful management of storage, write overhead, and transaction isolation to mitigate its potential drawbacks.

---

### e) What are Common Table Expressions (CTEs)?

Common Table Expressions (CTEs) are a powerful feature in SQL that allow you to define temporary result sets that can be referenced within a `SELECT`, `INSERT`, `UPDATE`, or `DELETE` statement. CTEs improve the readability and maintainability of complex queries by breaking them down into simpler, more manageable parts.

### Key Characteristics of CTEs:
1. **Temporary Result Sets**:
   - A CTE is defined using the `WITH` clause and exists only during the execution of the query.
   - It is not stored as an object in the database and is discarded after the query execution completes.

2. **Scope**:
   - The scope of a CTE is limited to the query in which it is defined. It cannot be referenced outside of that query.

3. **Recursive Queries**:
   - CTEs can be recursive, meaning they can reference themselves. This is particularly useful for hierarchical or tree-structured data, such as organizational charts or bill-of-materials.

### Syntax of a CTE:
```sql
WITH cte_name (column1, column2, ...) AS (
    -- CTE query definition
    SELECT column1, column2, ...
    FROM some_table
    WHERE some_condition
)
-- Main query that uses the CTE
SELECT *
FROM cte_name;
```

### Example of a Simple CTE:
```sql
WITH Sales_CTE AS (
    SELECT SalesPersonID, SUM(SalesAmount) AS TotalSales
    FROM Sales
    GROUP BY SalesPersonID
)
SELECT SalesPersonID, TotalSales
FROM Sales_CTE
WHERE TotalSales > 100000;
```

### Example of a Recursive CTE:
```sql
WITH Recursive_CTE AS (
    -- Anchor member
    SELECT EmployeeID, ManagerID, EmployeeName
    FROM Employees
    WHERE ManagerID IS NULL
    UNION ALL
    -- Recursive member
    SELECT e.EmployeeID, e.ManagerID, e.EmployeeName
    FROM Employees e
    INNER JOIN Recursive_CTE r ON e.ManagerID = r.EmployeeID
)
SELECT *
FROM Recursive_CTE;
```

### Advantages of Using CTEs:
1. **Improved Readability**:
   - CTEs make complex queries easier to read and understand by breaking them into logical parts.

2. **Modularity**:
   - CTEs allow you to modularize your queries, making them easier to maintain and debug.

3. **Reusability**:
   - Within the same query, you can reference a CTE multiple times, avoiding the need to repeat the same subquery.

4. **Recursive Queries**:
   - CTEs support recursion, which is essential for querying hierarchical data structures.

### Limitations of CTEs:
1. **Scope**:
   - CTEs are only available within the scope of the query in which they are defined. They cannot be reused in other queries.

2. **Performance**:
   - While CTEs can improve readability, they do not always improve performance. In some cases, the database optimizer may not handle CTEs as efficiently as subqueries or temporary tables.

3. **Materialization**:
   - CTEs are not materialized by default (i.e., they are not stored as temporary tables). This means that if a CTE is referenced multiple times in a query, the underlying query may be executed multiple times.

In summary, Common Table Expressions (CTEs) are a valuable tool in SQL for simplifying complex queries, improving readability, and enabling recursive queries. However, they should be used judiciously, considering their scope and potential performance implications.

---

## 2. Free Exploration

TODO

## 3. Reflection & Wrap-up

> ! TODO: Please, check and confirm. Done with DeepSeek!!!!!

### a) What did you observe when using pg locks in PostgreSQL? How did different isolation levels (e.g., READ COMMITTED vs. SERIALIZABLE) affect concurrency anomalies like dirty reads, non-repeatable reads, and phantoms?

When using `pg_locks` in PostgreSQL to observe locking behavior and the effects of different isolation levels, several key observations can be made regarding concurrency anomalies such as dirty reads, non-repeatable reads, and phantom reads. Here's a detailed analysis:

### Observations Using `pg_locks`:
1. **Lock Types**:
   - PostgreSQL uses various lock types (e.g., `RowShareLock`, `RowExclusiveLock`, `ShareLock`, `ExclusiveLock`) to manage concurrent access to data.
   - `pg_locks` provides a view of the current locks held by transactions, allowing you to monitor lock contention and blocking scenarios.

2. **Lock Contention**:
   - High lock contention can be observed in `pg_locks` when multiple transactions attempt to access the same data concurrently.
   - Blocking situations occur when one transaction holds a lock that another transaction is waiting for, leading to potential performance bottlenecks.

### Effects of Isolation Levels:

1. **READ COMMITTED**:
   - **Dirty Reads**: Not possible. PostgreSQL ensures that a transaction only sees committed data.
   - **Non-Repeatable Reads**: Possible. If a transaction reads the same row twice, it may see different values if another transaction commits changes in between.
   - **Phantom Reads**: Possible. New rows added by other transactions can appear in subsequent reads within the same transaction.

   **Example**:
   ```sql
   -- Transaction 1
   BEGIN;
   SELECT * FROM accounts WHERE balance > 1000; -- First read

   -- Transaction 2
   BEGIN;
   UPDATE accounts SET balance = balance - 100 WHERE id = 1;
   COMMIT;

   -- Transaction 1
   SELECT * FROM accounts WHERE balance > 1000; -- Second read may show different results
   COMMIT;
   ```

2. **REPEATABLE READ**:
   - **Dirty Reads**: Not possible.
   - **Non-Repeatable Reads**: Not possible. The transaction sees a consistent snapshot of the data as of the start of the transaction.
   - **Phantom Reads**: Possible. New rows added by other transactions can still appear.

   **Example**:
   ```sql
   -- Transaction 1
   BEGIN ISOLATION LEVEL REPEATABLE READ;
   SELECT * FROM accounts WHERE balance > 1000; -- First read

   -- Transaction 2
   BEGIN;
   INSERT INTO accounts (id, balance) VALUES (3, 1500);
   COMMIT;

   -- Transaction 1
   SELECT * FROM accounts WHERE balance > 1000; -- Second read may show new rows
   COMMIT;
   ```

3. **SERIALIZABLE**:
   - **Dirty Reads**: Not possible.
   - **Non-Repeatable Reads**: Not possible.
   - **Phantom Reads**: Not possible. PostgreSQL uses predicate locking to ensure that no new rows can appear that would affect the transaction's view of the data.

   **Example**:
   ```sql
   -- Transaction 1
   BEGIN ISOLATION LEVEL SERIALIZABLE;
   SELECT * FROM accounts WHERE balance > 1000; -- First read

   -- Transaction 2
   BEGIN;
   INSERT INTO accounts (id, balance) VALUES (3, 1500);
   COMMIT; -- This will be blocked or result in a serialization failure

   -- Transaction 1
   SELECT * FROM accounts WHERE balance > 1000; -- Second read will be consistent
   COMMIT;
   ```

### Summary of Isolation Levels and Anomalies:
- **READ COMMITTED**:
  - Prevents dirty reads.
  - Allows non-repeatable reads and phantom reads.

- **REPEATABLE READ**:
  - Prevents dirty reads and non-repeatable reads.
  - Allows phantom reads.

- **SERIALIZABLE**:
  - Prevents dirty reads, non-repeatable reads, and phantom reads.
  - Provides the highest level of isolation but may lead to more serialization failures and retries.

### Practical Considerations:
- **Performance**: Higher isolation levels (e.g., SERIALIZABLE) can lead to increased lock contention and potential serialization failures, impacting performance.
- **Application Logic**: Choose the appropriate isolation level based on the application's requirements for consistency and performance.
- **Monitoring**: Use `pg_locks` and other PostgreSQL monitoring tools to observe and tune locking behavior and concurrency.

In summary, using `pg_locks` in PostgreSQL helps observe locking behavior and the impact of different isolation levels on concurrency anomalies. Understanding these effects allows for better tuning and management of concurrent transactions in a database system.

---

### b) How does version control (e.g., MVCC in PostgreSQL) improve concurrency compared to traditional locking mechanisms? What trade-offs did you notice when working with version-controlled transactions?

Version control mechanisms, such as Multi-Version Concurrency Control (MVCC) used in PostgreSQL, significantly improve concurrency compared to traditional locking mechanisms. Here's how MVCC enhances concurrency and the trade-offs involved:

### How MVCC Improves Concurrency:

1. **Non-Blocking Reads**:
   - **Readers and Writers**: MVCC allows readers to access a consistent snapshot of the database without blocking writers, and vice versa. This eliminates the contention between read and write operations.
   - **Consistent Snapshots**: Each transaction sees a snapshot of the database as it was at the start of the transaction, ensuring consistency without the need for locks.

2. **Reduced Lock Contention**:
   - **Minimized Locking**: MVCC reduces the need for locks by maintaining multiple versions of data items. Writers create new versions, while readers access the appropriate version based on their snapshot.
   - **Lower Lock Management Overhead**: The overhead associated with acquiring, releasing, and managing locks is significantly reduced, leading to better performance in highly concurrent environments.

3. **Improved Performance in Read-Heavy Workloads**:
   - **Read-Only Transactions**: Read-only transactions can proceed without any locking overhead, making MVCC particularly efficient for workloads with a high proportion of read operations.
   - **Scalability**: MVCC scales better with increasing numbers of concurrent transactions, as it avoids the bottlenecks associated with lock contention.

4. **Support for Various Isolation Levels**:
   - **Flexibility**: MVCC supports different isolation levels (e.g., Read Committed, Repeatable Read, Serializable), allowing applications to balance consistency and performance based on their requirements.

### Trade-offs of Version-Controlled Transactions:

1. **Storage Overhead**:
   - **Multiple Versions**: MVCC maintains multiple versions of data items, which can lead to increased storage requirements. Old versions must be retained until no active transactions need them.
   - **Garbage Collection**: Efficient garbage collection mechanisms are required to clean up old versions, which can add complexity and overhead.

2. **Write Amplification**:
   - **Version Creation**: Each write operation creates a new version of the data item, which can lead to write amplification, especially in write-intensive workloads.
   - **Index Maintenance**: Indexes may need to be updated to reflect new versions, adding to the write overhead.

3. **Complexity**:
   - **Implementation**: MVCC is more complex to implement compared to traditional locking mechanisms. It requires careful management of versions, snapshots, and garbage collection.
   - **Conflict Detection**: Detecting and resolving conflicts, especially in higher isolation levels, can add to the complexity.

4. **Potential for Longer Transaction Times**:
   - **Version Retention**: Long-running transactions can cause old versions to be retained for extended periods, increasing storage requirements and potentially impacting performance.
   - **Snapshot Maintenance**: Maintaining consistent snapshots for long-running transactions can be challenging and may require additional mechanisms to ensure correctness.

5. **Isolation Level Trade-offs**:
   - **Weaker Isolation**: While MVCC provides strong consistency guarantees, some isolation levels (e.g., Read Committed) may allow anomalies that stricter locking mechanisms would prevent.
   - **Serializable Isolation**: Achieving serializable isolation with MVCC can be complex and may require additional mechanisms like predicate locking or conflict detection.

### Summary of Trade-offs:
- **Advantages**: MVCC offers improved concurrency, consistent snapshots, reduced locking overhead, and better performance in read-heavy workloads.
- **Drawbacks**: It can lead to increased storage overhead, write amplification, implementation complexity, potential for longer transaction times, and trade-offs in isolation levels.

### Practical Considerations:
- **Workload Characteristics**: MVCC is particularly beneficial for read-heavy workloads but may require careful tuning for write-intensive scenarios.
- **Storage Management**: Efficient garbage collection and storage management are crucial to mitigate the overhead of maintaining multiple versions.
- **Isolation Levels**: Choose the appropriate isolation level based on the application's requirements for consistency and performance.

In summary, MVCC significantly improves concurrency by allowing non-blocking reads and reducing lock contention, but it comes with trade-offs related to storage overhead, write amplification, and complexity. Understanding these trade-offs helps in effectively leveraging MVCC for optimal database performance and consistency.

---

### c) When using deferred constraints and savepoints, how do they affect transaction integrity and rollback operations? Give an example where a deferred constraint ensures consistency in a complex transaction.

Deferred constraints and savepoints are powerful features in SQL that help manage transaction integrity and rollback operations, especially in complex transactions. Here's how they affect transaction integrity and rollback operations, along with an example illustrating their use:

### Deferred Constraints:
Deferred constraints are constraints (e.g., foreign key or unique constraints) that are not enforced immediately when a statement is executed but are instead checked at the end of the transaction. This allows for more flexibility in complex transactions where temporary violations of constraints might occur.

### Savepoints:
Savepoints allow you to set markers within a transaction to which you can roll back without affecting the entire transaction. This provides a way to handle partial failures and recover gracefully within a transaction.

### Effects on Transaction Integrity and Rollback Operations:

1. **Deferred Constraints**:
   - **Integrity**: Deferred constraints ensure that all constraints are satisfied at the end of the transaction, allowing temporary violations during the transaction.
   - **Rollback**: If a deferred constraint is violated at the end of the transaction, the entire transaction is rolled back, ensuring consistency.

2. **Savepoints**:
   - **Integrity**: Savepoints allow you to maintain transaction integrity by providing a way to roll back to a specific point within the transaction, preserving the work done up to that point.
   - **Rollback**: Rolling back to a savepoint undoes all changes made after the savepoint was established, but changes made before the savepoint remain intact.

### Example: Deferred Constraint Ensuring Consistency

Consider a scenario where you need to transfer funds between two accounts and update a transaction log. The transaction must ensure that the total balance remains consistent and that the transaction log is updated atomically.

```sql
-- Create tables
CREATE TABLE accounts (
    account_id SERIAL PRIMARY KEY,
    balance DECIMAL NOT NULL
);

CREATE TABLE transactions (
    transaction_id SERIAL PRIMARY KEY,
    from_account_id INT REFERENCES accounts(account_id) DEFERRABLE INITIALLY DEFERRED,
    to_account_id INT REFERENCES accounts(account_id) DEFERRABLE INITIALLY DEFERRED,
    amount DECIMAL NOT NULL
);

-- Insert initial data
INSERT INTO accounts (account_id, balance) VALUES (1, 1000), (2, 500);

-- Begin transaction
BEGIN;

-- Set a savepoint
SAVEPOINT before_transfer;

-- Transfer funds
UPDATE accounts SET balance = balance - 200 WHERE account_id = 1;
UPDATE accounts SET balance = balance + 200 WHERE account_id = 2;

-- Log the transaction
INSERT INTO transactions (from_account_id, to_account_id, amount) VALUES (1, 2, 200);

-- Check for constraint violations
-- If any constraint is violated, rollback to the savepoint
-- For example, if the balance goes negative, it will be caught by the deferred constraint
-- Assuming a check constraint on balance >= 0
ALTER TABLE accounts ADD CONSTRAINT positive_balance CHECK (balance >= 0) DEFERRABLE INITIALLY DEFERRED;

-- Commit the transaction
COMMIT;
```

### Explanation:
1. **Deferred Constraints**:
   - The foreign key constraints on `transactions` table are deferred, meaning they are checked only at the end of the transaction.
   - The check constraint on `accounts.balance` is also deferred, ensuring that the balance does not go negative only at the end of the transaction.

2. **Savepoints**:
   - A savepoint `before_transfer` is set before the fund transfer and transaction logging.
   - If any constraint is violated (e.g., balance goes negative), the transaction can roll back to the savepoint, undoing the fund transfer and transaction logging while preserving the initial state.

3. **Transaction Integrity**:
   - The use of deferred constraints ensures that all constraints are satisfied at the end of the transaction, maintaining consistency.
   - Savepoints provide a way to handle partial failures and recover gracefully within the transaction.

### Summary:
- **Deferred Constraints**: Ensure that constraints are checked at the end of the transaction, allowing temporary violations and ensuring consistency.
- **Savepoints**: Allow partial rollback within a transaction, preserving the work done up to the savepoint and handling partial failures gracefully.

By using deferred constraints and savepoints, you can manage complex transactions more effectively, ensuring data integrity and providing robust rollback mechanisms.

--- 

### d) What benefits did you observe when using Common Table Expressions (CTEs) instead of subqueries? How did recursive CTEs help when working with hierarchical data?

Using Common Table Expressions (CTEs) instead of subqueries offers several benefits, particularly in terms of readability, maintainability, and performance in certain scenarios. Additionally, recursive CTEs are particularly useful for working with hierarchical data. Here’s a detailed look at the benefits and use cases:

### Benefits of Using CTEs Instead of Subqueries:

1. **Improved Readability**:
   - **Modularity**: CTEs allow you to break down complex queries into simpler, more manageable parts. Each part can be defined and understood independently.
   - **Self-Documenting**: CTEs can be given meaningful names, making the query easier to read and understand.

2. **Reusability**:
   - **Multiple References**: Within the same query, you can reference a CTE multiple times without having to repeat the subquery. This reduces redundancy and potential errors.

3. **Maintainability**:
   - **Easier Debugging**: Since CTEs are defined separately, it’s easier to test and debug individual parts of the query.
   - **Simpler Modifications**: Changes to a CTE are localized, making it easier to modify the query without affecting other parts.

4. **Performance**:
   - **Optimization**: In some cases, the database optimizer can handle CTEs more efficiently than subqueries, especially when the same subquery is used multiple times.

### Example: CTE vs. Subquery

**Subquery Example**:
```sql
SELECT employee_id, name, department_id
FROM employees
WHERE department_id IN (
    SELECT department_id
    FROM departments
    WHERE location_id = 1700
);
```

**CTE Example**:
```sql
WITH Department_CTE AS (
    SELECT department_id
    FROM departments
    WHERE location_id = 1700
)
SELECT employee_id, name, department_id
FROM employees
WHERE department_id IN (SELECT department_id FROM Department_CTE);
```

### Recursive CTEs for Hierarchical Data:

Recursive CTEs are particularly powerful for working with hierarchical or tree-structured data, such as organizational charts, bill-of-materials, or category hierarchies.

### Benefits of Recursive CTEs:

1. **Handling Hierarchies**:
   - **Traversal**: Recursive CTEs allow you to traverse hierarchical data structures, such as finding all descendants of a node or all ancestors of a node.
   - **Depth Control**: You can control the depth of recursion, making it possible to limit the traversal to a certain number of levels.

2. **Simplicity**:
   - **Single Query**: Recursive CTEs allow you to handle hierarchical data in a single query, avoiding the need for multiple queries or procedural code.

3. **Flexibility**:
   - **Dynamic Results**: Recursive CTEs can generate dynamic results based on the hierarchical structure, such as generating paths or levels.

### Example: Recursive CTE for Hierarchical Data

Consider an `employees` table where each employee has a `manager_id` that references another employee, creating a hierarchy.

```sql
CREATE TABLE employees (
    employee_id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    manager_id INT REFERENCES employees(employee_id)
);

-- Insert sample data
INSERT INTO employees (employee_id, name, manager_id) VALUES
(1, 'Alice', NULL),
(2, 'Bob', 1),
(3, 'Charlie', 1),
(4, 'David', 2),
(5, 'Eve', 2),
(6, 'Frank', 3);

-- Recursive CTE to find all subordinates of a given manager
WITH RECURSIVE Subordinates_CTE AS (
    -- Anchor member: start with the given manager
    SELECT employee_id, name, manager_id
    FROM employees
    WHERE employee_id = 1
    UNION ALL
    -- Recursive member: find subordinates of the current level
    SELECT e.employee_id, e.name, e.manager_id
    FROM employees e
    INNER JOIN Subordinates_CTE s ON e.manager_id = s.employee_id
)
SELECT * FROM Subordinates_CTE;
```

### Explanation:
1. **Anchor Member**: The initial query selects the root of the hierarchy (e.g., the manager with `employee_id = 1`).
2. **Recursive Member**: The recursive part joins the `employees` table with the CTE to find subordinates of the current level.
3. **Result**: The CTE returns all employees who are direct or indirect subordinates of the given manager.

### Summary:
- **CTEs**: Improve readability, maintainability, and reusability compared to subqueries. They can also offer performance benefits in some cases.
- **Recursive CTEs**: Provide a powerful and flexible way to work with hierarchical data, allowing for easy traversal and dynamic result generation.

By leveraging CTEs and recursive CTEs, you can write more readable, maintainable, and efficient SQL queries, especially when dealing with complex or hierarchical data structures.

---

### e) Reflecting on the hands-on exercises, what challenges did you face when implementing CTE queries, and how did you overcome them? How do CTEs impact query performance in PostgreSQL?

Implementing Common Table Expressions (CTEs) in PostgreSQL can be highly beneficial, but it also comes with its own set of challenges. Here are some common challenges faced when working with CTEs and strategies to overcome them, along with insights into how CTEs impact query performance:

### Challenges and Solutions:

1. **Understanding Recursion**:
   - **Challenge**: Recursive CTEs can be conceptually difficult to understand, especially for those new to hierarchical data traversal.
   - **Solution**: Break down the recursive CTE into its anchor and recursive members. Start with simple examples and gradually increase complexity. Use comments and meaningful names to make the logic clearer.

2. **Performance Issues**:
   - **Challenge**: CTEs, especially recursive ones, can sometimes lead to performance issues if not optimized properly.
   - **Solution**: Ensure that the base tables involved in the CTE are properly indexed. Use `EXPLAIN ANALYZE` to understand the query plan and identify bottlenecks. Consider materializing intermediate results if necessary.

3. **Complex Query Logic**:
   - **Challenge**: Complex queries with multiple CTEs can become hard to read and maintain.
   - **Solution**: Modularize the query by breaking it down into smaller, logical CTEs. Use descriptive names for each CTE to improve readability. Test each CTE independently before combining them.

4. **Data Volume**:
   - **Challenge**: Large datasets can lead to performance degradation, especially with recursive CTEs that may generate a large number of rows.
   - **Solution**: Limit the depth of recursion if possible. Use filtering conditions to reduce the number of rows processed. Consider alternative approaches like iterative processing in application code if the dataset is extremely large.

### Impact on Query Performance:

1. **Execution Plan**:
   - **CTEs in PostgreSQL**: By default, CTEs in PostgreSQL are optimization fences, meaning the planner does not inline the CTE into the main query. This can lead to suboptimal execution plans.
   - **Materialized CTEs**: PostgreSQL treats CTEs as materialized by default, which means the result of the CTE is computed once and stored in memory. This can be beneficial if the CTE is referenced multiple times but can be a drawback if the CTE result is large.

2. **Optimization Tips**:
   - **Inline CTEs**: Use the `WITH` clause with the `MATERIALIZED` or `NOT MATERIALIZED` hint to control whether the CTE is materialized. For example:
     ```sql
     WITH cte_name AS NOT MATERIALIZED (
         SELECT * FROM some_table
     )
     SELECT * FROM cte_name;
     ```
   - **Indexes**: Ensure that the underlying tables used in the CTE have appropriate indexes to speed up access.
   - **Filter Early**: Apply filters as early as possible in the CTE to reduce the number of rows processed.

### Example: Performance Consideration

Consider a recursive CTE that traverses a hierarchical employee table:

```sql
WITH RECURSIVE Subordinates_CTE AS (
    SELECT employee_id, name, manager_id
    FROM employees
    WHERE manager_id IS NULL
    UNION ALL
    SELECT e.employee_id, e.name, e.manager_id
    FROM employees e
    INNER JOIN Subordinates_CTE s ON e.manager_id = s.employee_id
)
SELECT * FROM Subordinates_CTE;
```

### Performance Analysis:
1. **Indexes**: Ensure that `manager_id` is indexed to speed up the join operation.
2. **Filtering**: If you only need a subset of the hierarchy, add a filter condition in the anchor member or recursive member to limit the number of rows.
3. **Execution Plan**: Use `EXPLAIN ANALYZE` to understand the query plan and identify any performance bottlenecks.

### Example: Using `EXPLAIN ANALYZE`
```sql
EXPLAIN ANALYZE
WITH RECURSIVE Subordinates_CTE AS (
    SELECT employee_id, name, manager_id
    FROM employees
    WHERE manager_id IS NULL
    UNION ALL
    SELECT e.employee_id, e.name, e.manager_id
    FROM employees e
    INNER JOIN Subordinates_CTE s ON e.manager_id = s.employee_id
)
SELECT * FROM Subordinates_CTE;
```

### Summary:
- **Challenges**: Understanding recursion, managing performance, handling complex logic, and dealing with large datasets are common challenges when implementing CTEs.
- **Solutions**: Break down complex queries, use indexes, filter early, and analyze execution plans to optimize performance.
- **Performance Impact**: CTEs can impact query performance, especially if not optimized. Use `MATERIALIZED` or `NOT MATERIALIZED` hints, ensure proper indexing, and analyze execution plans to mitigate performance issues.

By addressing these challenges and understanding the performance implications, you can effectively leverage CTEs to write more readable, maintainable, and efficient SQL queries in PostgreSQL.

